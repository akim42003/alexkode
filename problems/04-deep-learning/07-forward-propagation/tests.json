{
    "function": "solution",
    "test_cases": [
        {
            "name": "single ReLU layer",
            "input": {"X": [[1.0, 2.0]], "weights": [[[0.1, 0.2], [0.3, 0.4]]], "biases": [[0.5, 0.6]], "activations": ["relu"]},
            "expected": [[1.2, 1.6]]
        },
        {
            "name": "linear identity",
            "input": {"X": [[1.0, 2.0]], "weights": [[[1, 0], [0, 1]]], "biases": [[0, 0]], "activations": ["linear"]},
            "expected": [[1.0, 2.0]]
        },
        {
            "name": "sigmoid output",
            "input": {"X": [[0.0]], "weights": [[[1.0]]], "biases": [[0.0]], "activations": ["sigmoid"]},
            "expected": [[0.5]]
        },
        {
            "name": "relu zeros negatives",
            "input": {"X": [[1.0, -1.0]], "weights": [[[1, 0], [0, 1]]], "biases": [[0, 0]], "activations": ["relu"]},
            "expected": [[1.0, 0.0]]
        },
        {
            "name": "softmax output sums to 1",
            "input": {"X": [[1.0, 2.0, 3.0]], "weights": [[[1, 0, 0], [0, 1, 0], [0, 0, 1]]], "biases": [[0, 0, 0]], "activations": ["softmax"]},
            "expected": [[0.0900305732, 0.2447284711, 0.6652409557]]
        }
    ]
}
